\paragraph{Motivation.}
In clinical settings, the cost of a generation error is rarely ``unnatural phrasing''---it is an unsupported claim that can trigger downstream mis-triage.
This risk is amplified in 3D CT: volumes are information-dense but also highly redundant, and na\"ive tokenization schemes (e.g., fixed grids over all voxels) are computationally expensive.
As a result, grounded report generation is inherently a \emph{budget allocation} problem: under a strict inference budget, which spatial regions deserve encoding, which statements must cite evidence, and when should the system refuse due to insufficient support?

\paragraph{Why prior approaches are hard to audit.}
Existing 3D report generation work has strengthened datasets and baselines for the task~\citep{hamamci2024developing,hamamci2024ct2rep,hamamci2023generatect}, but the dominant evaluation focus remains ``text looks correct'' rather than ``each clinical statement is supported by localized evidence.''
Separately, pixel-level grounding datasets make sentence-to-mask evaluation possible~\citep{baharoon2025rexgroundingct}, yet grounding is often evaluated post-hoc rather than enforced by the generation protocol.
Finally, trustworthy generation and RAG research increasingly emphasizes attribution quality and learning to refuse~\citep{song2024trustalign}, but refusal can appear to improve safety by over-refusing unless calibrated under explicit miss-rate constraints.
These pieces are rarely tied together under one matched-cost, end-to-end, scriptable audit.

\paragraph{Our reframing: proof-carrying generation under a budget.}
We propose \ProveTok, which rewrites 3D CT report generation as a coupled problem of \emph{budgeted evidence selection} and \emph{proof-carrying generation}.
We define a unified compute budget $\Btotal=\Benc+\Bgen$ and an output contract consisting of \emph{frames}, \emph{citations}, \emph{refusal}, and a \emph{verifier trace}.
Here, a \emph{frame} is an atomic, typed clinical statement (e.g., a finding assertion) that is required to cite evidence tokens with explicit 3D support regions $\Omega$.
Groundedness is not an after-the-fact visualization: every generated frame must ``carry its proof'' (citations + verifier trace) or be rejected by a verifier that triggers refinement or calibrated refusal.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{fig1_system_overview.png}
  \caption{\textbf{\ProveTok overview.} Under a unified budget $\Btotal=\Benc+\Bgen$, BET allocates encoder compute $\Benc$ to produce evidence tokens with explicit 3D support regions $\Omega$, and PCG allocates decoder compute $\Bgen$ to generate report statements with mandatory citations, verifier traces, and calibrated refusal.}
  \label{fig:system}
\end{figure*}

\paragraph{Contributions.}
We emphasize protocol and auditability over yet another backbone.
Our contributions are:
\begin{itemize}
  \item \textbf{Proof-carrying protocol under matched budgets.} We introduce BET and PCG as a unified contract that makes evidence, refusal, and latency jointly auditable under $\Btotal=\Benc+\Bgen$ (Figure~\ref{fig:system}).
  \item \textbf{Claim-level proof rules and statistics.} We implement scriptable claim-level tests with multi-budget, multi-seed evaluation using paired bootstrap and Holm correction (Table~\ref{tab:oral-minset}).
  \item \textbf{Decisive evidence via counterfactual and calibration.} We provide counterfactual stress tests showing non-trivial citation effects and calibrated refusal with explicit anti-silencing gates (Figures~\ref{fig:counterfactual} and~\ref{fig:refusal}).
\end{itemize}

\paragraph{Paper roadmap.}
Section~\ref{sec:method} defines the budgeted proof-carrying protocol; Section~\ref{sec:exp} describes matched-cost experiments and statistical tests; Section~\ref{sec:results} presents main evidence and stress tests.
