\label{sec:method}

\subsection{Problem Setup}
Let $V$ be a 3D CT volume and let the model generate a report consisting of clinical statements.
We define a unified inference budget
\begin{equation}
  \Btotal = \Benc + \Bgen,
\end{equation}
where $\Benc$ bounds the compute used for evidence tokenization/selection and $\Bgen$ bounds the compute used for proof-carrying generation and verifier interactions.
The system output is a structured object containing (i) textual frames/statements, (ii) citations to evidence tokens, (iii) a refusal/uncertainty decision when evidence is insufficient, and (iv) a verifier trace that makes failures machine-auditable.
Formally, BET produces evidence tokens $\{t_i\}$, where each token is associated with an explicit 3D support region $\Omega_i$ and metadata (e.g., resolution level and score).
PCG generates frames $\{y_k\}$ and citations $C_k \subseteq \{t_i\}$, and a verifier $g(y_k, C_k)$ returns a pass/fail decision plus a failure type used for refinement or refusal.

\paragraph{Evidence cells and support regions.}
We represent spatial evidence with a multi-resolution dyadic grid.
A \emph{cell} $c=(\ell, i_x, i_y, i_z)$ at level $\ell$ partitions the volume into $2^\ell$ bins per axis, yielding an octree-like refinement family.
We use a deterministic mapping $\phi$ that maps a cell id to voxel indices, defining the support region $\Omega(c)=\phi(c; \mathrm{shape}(V)) \subseteq [1..D]\times[1..H]\times[1..W]$.
An evidence token is a tuple $t_i=(e_i, c_i, \ell_i, s_i, u_i)$ where $e_i$ is an embedding pooled over $\Omega(c_i)$ from a 3D encoder feature map (mean pooling in our scaffold), $s_i\in[0,1]$ is an evidence score, and $u_i$ is an uncertainty proxy.
This representation makes citations machine-checkable: a citation points to a token id, which deterministically implies a 3D support region.

\subsection{BET: Budgeted Evidence Tokenization}
BET constructs a set of evidence tokens $\{t_i\}$ under budget $\Benc$.
Conceptually, BET answers: \emph{which spatial regions are worth spending encoder compute on}, when each additional token consumes part of $\Benc$?
In our implementation, encoding one active cell yields one token; budgets are expressed either directly as a token count ($b_{\mathrm{enc}}$) or via FLOPs/latency-matched accounting (Section~\ref{sec:exp}).

\paragraph{Full-cover initialization to avoid early misses.}
A greedy refinement policy that starts from a single root cell can ``commit'' too early and miss lesions entirely under strict budgets.
We therefore initialize with a full-covering grid at an initial level $\ell_0$ such that $|{\mathcal C}_0|=(2^{\ell_0})^3 \le b_{\mathrm{enc}}$, and only then refine within this grid until reaching the budget.

\paragraph{Refinement loop.}
At step $s$, given the current active cell set ${\mathcal C}_s$, we (i) encode tokens for all $c\in{\mathcal C}_s$, (ii) run PCG to produce frames and citations, (iii) run the verifier to obtain a typed issue list with evidence traces, and (iv) select one cell $c^\star\in{\mathcal C}_s$ to split into its eight children.
The selection is driven by either:
\begin{itemize}
  \item \textbf{Verifier-driven greedy.} If issues exist, prioritize cells referenced by the verifier evidence traces for failing frames (severity-weighted); otherwise split the cell with highest uncertainty $u$.
  \item \textbf{EvidenceHead ranking.} A small head predicts an expected issue reduction $\Delta(c)$ from the cell embedding and the current issue context, and we split the cell maximizing $\Delta(c)$ with a small $\epsilon$-greedy exploration.
\end{itemize}
This makes allocation auditable: when refinement happens, the system can point to which verifier failure type triggered it and which cited cells were implicated.

\paragraph{Theoretical view: submodular coverage under a budget.}
To make BET more than an interface, we cast evidence tokenization as a budgeted set selection problem.
Let $\mathcal{R}$ be a discrete family of candidate evidence regions (e.g., dyadic cells at multiple resolutions, or ROI proposals), where each region $r\in\mathcal{R}$ deterministically maps to a support region $\Omega_r$.
Let $\mathcal{U}$ be a set of utility elements (voxels, supervoxels, or finding anchors) with non-negative weights $w(u)$ (e.g., derived from a fixed saliency model or uncertainty map).
Given a token set $T\subseteq \mathcal{R}$, define a weighted coverage objective:
\begin{equation}
  F(T)=\sum_{u\in \mathcal{U}} w(u)\cdot \mathbf{1}\big[\exists r\in T: u\in \Omega_r\big].
\end{equation}
More generally, a facility-location-style variant can replace the indicator with a soft coverage strength $a(u,r)\in[0,1]$ via $F(T)=\sum_u w(u)\max_{r\in T} a(u,r)$; both forms are monotone submodular.
\paragraph{Proposition 1 (Uniform-cost greedy).}
Under a uniform-cost constraint $|T|\le K$, the classical greedy algorithm that repeatedly adds the region with the largest marginal gain achieves a $(1-1/e)$ approximation to the optimum for monotone submodular $F$~\citep{nemhauser1978analysis}.
\paragraph{Proposition 2 (Knapsack-style budgets).}
Under non-uniform token costs and a knapsack constraint, $(1-1/e)$-type approximation algorithms exist for monotone submodular maximization~\citep{sviridenko2004note}.
Our refinement loop is a practical, auditable greedy variant over a structured candidate family (dyadic splits): each split increases resolution locally and selects the next region using a verifier-driven proxy for marginal value (issue reduction and uncertainty).

\subsection{PCG: Proof-Carrying Generation}
PCG generates statements under budget $\Bgen$ with a hard contract: every asserted statement must cite evidence tokens.
We use a structured output space for auditability: the report is emitted as a set of typed \emph{frames} $y_k$ (finding type + slots such as polarity and laterality) together with citations $C_k$ and an accept probability $q_k$.

\paragraph{Frames and mandatory citations.}
PCG uses query-based attention from a small set of learnable finding queries to token embeddings, producing per-frame features and attention weights.
The top-$k$ attended tokens (or a score-interleaved variant) are recorded as citations $C_k$.
Slot classifiers then read the frame features to generate the typed statement.

\paragraph{Support probability and refusal.}
PCG also outputs an accept probability $q_k\in[0,1]$ intended to represent \emph{evidence-backed support} for the frame (not necessarily clinical correctness).
In our scaffold, $q_k$ is predicted by a small MLP from the frame feature together with citation strength proxies (e.g., maximum cited score) and decoding uncertainty (e.g., slot entropy).
Frames with insufficient support are either refined (by requesting additional evidence tokens or revising the statement) or refused under a calibrated threshold (next subsection).

\subsection{Verifier Taxonomy}
We implement a lightweight, rule-based verifier that categorizes failures into machine-checkable types (e.g., missing citation, insufficient spatial coverage, citation irrelevance) and emits a trace.
The verifier is deterministic and fixed for a paper artifact (versioned), and it is not trained to agree with the generator.
Therefore, ``supportedness'' in our protocol is defined with respect to this verifier, and should not be conflated with clinical truth.

\paragraph{Unsupportedness and overclaim.}
For unsupportedness (U1), key checks include: positive claims must have citations; cited evidence must exceed minimum score / maximum uncertainty thresholds; and citations must cover non-trivial spatial extent.
We approximate cited coverage by the sum of dyadic volume fractions, $\sum_{t\in C_k} 8^{-\ell(t)}$, and flag insufficient coverage below a fixed ratio.
For citation relevance, we optionally score tokens with a deterministic query attention and require that citations recover the top-$k$ attended evidence with sufficient attention mass (Appendix).
Overclaim (O1) captures specificity mismatches, e.g., a laterality-specific statement backed only by coarse evidence levels.

\subsection{Refusal Calibration with Anti-Silencing Gates}
Each frame $y_k$ is assigned a support probability $q_k \in [0,1]$ intended to estimate \emph{evidence-backed support} (not necessarily clinical correctness).
We calibrate a refusal threshold $\tauRef$ on development data under explicit safety constraints, then freeze it for test evaluation.
Crucially, refusal is not allowed to ``game'' supportedness by staying silent: we enforce hard gates on (i) \emph{critical miss-rate} (critical findings present in ground-truth but incorrectly refused), (ii) refusal calibration error (ECE/reliability), and (iii) refusal rate.

We use a fixed, audited critical finding set (e.g., pneumothorax, effusion, consolidation, nodule) for anti-silencing accounting.
For ECE/reliability, we treat ``correct'' as ``supported by the verifier'' (i.e., no unsupported issues) on asserted positive frames, matching the semantics of $q_k$.

\subsection{Claim-level Proof Rules}
Rather than relying on cherry-picked qualitative examples, we define a bounded set of claims and implement scriptable pass/fail rules.
Each claim points to a reproducible artifact and a statistical protocol (paired bootstrap and Holm correction).
Table~\ref{tab:oral-minset} provides the minimal decisive evidence set for oral-level defensibility.
